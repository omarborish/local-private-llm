{
  "$schema": "https://schema.tauri.app/config/2",
  "productName": "Local Private LLM",
  "version": "0.1.0",
  "identifier": "com.localprivate.llm",
  "build": {
    "beforeDevCommand": "npm run dev",
    "devUrl": "http://localhost:1420",
    "beforeBuildCommand": "npm run build",
    "frontendDist": "../dist"
  },
  "app": {
    "withGlobalTauri": false,
    "windows": [
      {
        "title": "Local Private LLM",
        "width": 1100,
        "height": 700,
        "minWidth": 800,
        "minHeight": 500,
        "resizable": true,
        "fullscreen": false
      }
    ],
    "security": {
      "csp": null,
      "capabilities": ["default"]
    }
  },
  "bundle": {
    "active": true,
    "targets": ["msi", "nsis"],
    "icon": [
      "icons/32x32.png",
      "icons/128x128.png",
      "icons/128x128@2x.png",
      "icons/icon.icns",
      "icons/icon.ico"
    ],
    "windows": {
      "certificateThumbprint": null,
      "digestAlgorithm": "sha256",
      "timestampUrl": ""
    },
    "longDescription": "A free, privacy-first desktop AI assistant that runs 100% locally. Powered by open-weight models via Ollama. Features tool integration (file, web search, terminal), model management, and zero telemetry.",
    "shortDescription": "Local Private LLM â€” privacy-first AI assistant",
    "macOS": {
      "entitlements": null,
      "exceptionDomain": "",
      "frameworks": [],
      "providerShortName": null,
      "signingIdentity": null
    },
    "resources": [],
    "externalBin": [],
    "copyright": "MIT License",
    "category": "Productivity"
  },
  "plugins": {
    "shell": {
      "open": true
    }
  }
}
